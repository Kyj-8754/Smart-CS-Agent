"""
BíŒŒíŠ¸: ìºì‹œ ì‹œìŠ¤í…œì´ ì¶”ê°€ëœ RAG
- ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ìºì‹œ
- ê²€ì¦ëœ ë‹µë³€ ì¬ì‚¬ìš©
- LLM ë¹„ìš© ì ˆê°
"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from datetime import datetime
import logging
import os
import re
import time
import json
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================== ìºì‹œ ê´€ë¦¬ì (NEW!) ====================

class AnswerCache:
    """
    ê²€ì¦ëœ ë‹µë³€ ìºì‹œ ì‹œìŠ¤í…œ
    
    ê¸°ëŠ¥:
    1. ì§ˆë¬¸-ë‹µë³€ ìŒ ì €ì¥
    2. ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ìºì‹±
    3. ìºì‹œ íˆíŠ¸ ì‹œ ì¦‰ì‹œ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ìŒ)
    """
    
    def __init__(self, cache_file: str = "backend/data/answer_cache.json"):
        self.cache_file = Path(cache_file)
        self.cache = self._load_cache()
        self.embeddings_cache = {}  # ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ìºì‹œ
        logger.info(f"  âœ… ë‹µë³€ ìºì‹œ ì´ˆê¸°í™” ({len(self.cache)}ê°œ ì €ì¥ë¨)")
    
    def _load_cache(self) -> Dict:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        try:
            self.cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_query_hash(self, query: str, category: str = None) -> str:
        """ì§ˆë¬¸ì˜ í•´ì‹œê°’ ìƒì„±"""
        key = f"{category}:{query}" if category else query
        return hashlib.md5(key.encode()).hexdigest()
    
    def get(self, query: str, category: str = None, similarity_threshold: float = 0.95) -> Optional[Dict]:
        """
        ìºì‹œì—ì„œ ë‹µë³€ ì¡°íšŒ
        
        Args:
            query: ì§ˆë¬¸
            category: ì¹´í…Œê³ ë¦¬
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.95 - ë§¤ìš° ìœ ì‚¬í•´ì•¼ ìºì‹œ ì‚¬ìš©)
        
        Returns:
            ìºì‹œëœ ë‹µë³€ ë˜ëŠ” None
        """
        # ì •í™•íˆ ê°™ì€ ì§ˆë¬¸ ì°¾ê¸°
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            cached_item = self.cache[query_hash]
            
            # ìŠ¹ì¸ëœ ë‹µë³€ë§Œ ë°˜í™˜
            if cached_item.get('verified', False):
                logger.info(f"  ğŸ’¾ ìºì‹œ íˆíŠ¸! (ì •í™•í•œ ë§¤ì¹­)")
                cached_item['cache_hit'] = True
                cached_item['cache_type'] = 'exact'
                return cached_item
        
        # ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸° (ì„ë² ë”© ê¸°ë°˜)
        # TODO: ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ (ì„ íƒ ì‚¬í•­)
        
        return None
    
    def add(self, 
            query: str, 
            answer: str, 
            category: str = None,
            verified: bool = False,
            feedback_score: int = 0,
            metadata: Dict = None) -> str:
        """
        ìºì‹œì— ë‹µë³€ ì¶”ê°€
        
        Args:
            query: ì§ˆë¬¸
            answer: ë‹µë³€
            category: ì¹´í…Œê³ ë¦¬
            verified: ì‚¬ìš©ìê°€ ìŠ¹ì¸í–ˆëŠ”ì§€
            feedback_score: í”¼ë“œë°± ì ìˆ˜ (1-5)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ìºì‹œ í‚¤
        """
        query_hash = self._get_query_hash(query, category)
        
        self.cache[query_hash] = {
            'query': query,
            'answer': answer,
            'category': category,
            'verified': verified,
            'feedback_score': feedback_score,
            'created_at': datetime.now().isoformat(),
            'hit_count': 0,
            'metadata': metadata or {}
        }
        
        self._save_cache()
        logger.info(f"  ğŸ’¾ ìºì‹œ ì¶”ê°€: {query[:30]}... (verified={verified})")
        
        return query_hash
    
    def verify(self, query: str, category: str = None, feedback_score: int = 5):
        """
        ì‚¬ìš©ìê°€ ë‹µë³€ì„ ìŠ¹ì¸
        
        Args:
            query: ì§ˆë¬¸
            category: ì¹´í…Œê³ ë¦¬
            feedback_score: í”¼ë“œë°± ì ìˆ˜ (1-5)
        """
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            self.cache[query_hash]['verified'] = True
            self.cache[query_hash]['feedback_score'] = feedback_score
            self.cache[query_hash]['verified_at'] = datetime.now().isoformat()
            
            self._save_cache()
            logger.info(f"  âœ… ë‹µë³€ ìŠ¹ì¸: {query[:30]}... (ì ìˆ˜: {feedback_score})")
        else:
            logger.warning(f"  âš ï¸  ìºì‹œì— ì—†ëŠ” ì§ˆë¬¸: {query[:30]}...")
    
    def reject(self, query: str, category: str = None, reason: str = None):
        """
        ì‚¬ìš©ìê°€ ë‹µë³€ì„ ê±°ë¶€
        
        Args:
            query: ì§ˆë¬¸
            category: ì¹´í…Œê³ ë¦¬
            reason: ê±°ë¶€ ì´ìœ 
        """
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            # ê±°ë¶€ëœ ë‹µë³€ì€ ìºì‹œì—ì„œ ì œê±° ë˜ëŠ” ë§ˆí‚¹
            self.cache[query_hash]['verified'] = False
            self.cache[query_hash]['rejected'] = True
            self.cache[query_hash]['rejected_at'] = datetime.now().isoformat()
            self.cache[query_hash]['rejection_reason'] = reason
            
            self._save_cache()
            logger.info(f"  âŒ ë‹µë³€ ê±°ë¶€: {query[:30]}...")
    
    def increment_hit_count(self, query: str, category: str = None):
        """ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€"""
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            self.cache[query_hash]['hit_count'] = self.cache[query_hash].get('hit_count', 0) + 1
            self.cache[query_hash]['last_used'] = datetime.now().isoformat()
            self._save_cache()
    
    def get_stats(self) -> Dict:
        """ìºì‹œ í†µê³„"""
        total = len(self.cache)
        verified = sum(1 for item in self.cache.values() if item.get('verified'))
        rejected = sum(1 for item in self.cache.values() if item.get('rejected'))
        pending = total - verified - rejected
        
        total_hits = sum(item.get('hit_count', 0) for item in self.cache.values())
        
        return {
            'total_cached': total,
            'verified': verified,
            'rejected': rejected,
            'pending': pending,
            'total_cache_hits': total_hits,
            'cache_hit_rate': total_hits / max(total, 1)
        }


# ==================== ëŒ€í™” ë§¥ë½ ê´€ë¦¬ì ====================

class ConversationManager:
    """ëŒ€í™” ë§¥ë½ ê´€ë¦¬"""
    
    def __init__(self):
        self.sessions = {}
    
    def add_turn(self, session_id: str, user_query: str, bot_response: str, 
                 suggested_action: str = None, faq_ids: List[str] = None,
                 from_cache: bool = False):
        """ëŒ€í™” í„´ ì¶”ê°€"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {
                'history': [],
                'current_issue': None,
                'tried_solutions': [],
                'last_suggestion': None,
                'created_at': datetime.now()
            }
        
        self.sessions[session_id]['history'].append({
            'timestamp': datetime.now(),
            'user_query': user_query,
            'bot_response': bot_response,
            'suggested_action': suggested_action,
            'faq_ids': faq_ids or [],
            'from_cache': from_cache  # ìºì‹œì—ì„œ ì˜¨ ë‹µë³€ì¸ì§€
        })
        
        if suggested_action:
            self.sessions[session_id]['last_suggestion'] = suggested_action
            if suggested_action not in self.sessions[session_id]['tried_solutions']:
                self.sessions[session_id]['tried_solutions'].append(suggested_action)
        
        if not self.sessions[session_id]['current_issue']:
            self.sessions[session_id]['current_issue'] = self._extract_issue(user_query)
    
    def _extract_issue(self, query: str) -> str:
        """í˜„ì¬ ë¬¸ì œ ì¶”ì¶œ"""
        issues = {
            'ì¸í„°ë„·': 'internet_issue',
            'ì™€ì´íŒŒì´': 'wifi_issue',
            'ì•±': 'app_issue',
            'ëŠë¦¼': 'slow_issue',
            'ì²­êµ¬': 'billing_issue',
            'ì£¼ë¬¸': 'order_issue',
            'ë¡œê·¸ì¸': 'login_issue'
        }
        
        for keyword, issue in issues.items():
            if keyword in query:
                return issue
        return 'general_issue'
    
    def resolve_references(self, session_id: str, query: str) -> str:
        """ì§€ì‹œ ëŒ€ëª…ì‚¬ í•´ê²°"""
        if session_id not in self.sessions:
            return query
        
        context = self.sessions[session_id]
        if not context['last_suggestion']:
            return query
        
        references = {
            'ê·¸ê±°': context['last_suggestion'],
            'ê·¸ê²ƒ': context['last_suggestion'],
            'ì´ê±°': context['last_suggestion'],
            'ì´ê²ƒ': context['last_suggestion'],
        }
        
        resolved = query
        for ref, actual in references.items():
            if ref in resolved:
                resolved = resolved.replace(ref, actual)
                if actual not in context['tried_solutions']:
                    context['tried_solutions'].append(actual)
        
        if resolved != query:
            logger.info(f"[ë§¥ë½ í•´ê²°] '{query}' â†’ '{resolved}'")
        
        return resolved
    
    def build_context_prompt(self, session_id: str) -> str:
        """ëŒ€í™” ë§¥ë½ì„ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
        if session_id not in self.sessions:
            return ""
        
        context = self.sessions[session_id]
        if not context['tried_solutions']:
            return ""
        
        prompt = "\n[ì´ì „ ëŒ€í™” ë§¥ë½]\n"
        prompt += f"- í˜„ì¬ ë¬¸ì œ: {context['current_issue']}\n"
        prompt += f"- ê³ ê°ì´ ì´ë¯¸ ì‹œë„í•œ ë°©ë²•:\n"
        for i, solution in enumerate(context['tried_solutions'], 1):
            prompt += f"  {i}. {solution}\n"
        prompt += "\nâš ï¸ ìœ„ ë°©ë²•ë“¤ì€ ì´ë¯¸ ì‹œë„í–ˆìœ¼ë¯€ë¡œ ë‹¤ë¥¸ í•´ê²°ì±…ì„ ì œì•ˆí•˜ì„¸ìš”.\n\n"
        
        return prompt


# ==================== Agent (ì¬ì‹œë„ ë¡œì§) ====================

class LLMAgent:
    """LLM í˜¸ì¶œì„ ë‹´ë‹¹í•˜ëŠ” Agent"""
    
    def __init__(self, api_key: str = None, max_retries: int = 3):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.max_retries = max_retries
        self.client = None
        
        if self.api_key:
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=self.api_key)
                logger.info("  âœ… LLM Agent ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"  âŒ LLM Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def generate_with_retry(self, 
                           prompt: str, 
                           system_prompt: str = None,
                           temperature: float = 0.7,
                           max_tokens: int = 500) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” LLM í˜¸ì¶œ"""
        if not self.client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        system_prompt = system_prompt or self._get_default_system_prompt()
        
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.info(f"  ğŸ¤– LLM í˜¸ì¶œ ì‹œë„ {attempt}/{self.max_retries}")
                
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                answer = response.choices[0].message.content.strip()
                logger.info(f"  âœ… LLM í˜¸ì¶œ ì„±ê³µ (ê¸¸ì´: {len(answer)}ì)")
                
                return answer
                
            except Exception as e:
                logger.warning(f"  âš ï¸  LLM í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt}): {e}")
                
                if attempt < self.max_retries:
                    wait_time = 2 ** attempt
                    logger.info(f"  â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"  âŒ LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨")
                    raise Exception(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def _get_default_system_prompt(self) -> str:
        """ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ê³ ê° ì§€ì› AIì…ë‹ˆë‹¤.

ë‹µë³€ ê·œì¹™:
1. ê³ ê°ì´ ì´ë¯¸ ì‹œë„í•œ ë°©ë²•ì€ ë‹¤ì‹œ ì œì•ˆí•˜ì§€ ë§ˆì„¸ìš”
2. ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš” (1, 2, 3...)
3. ê¸°ìˆ  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
4. í•„ìš”ì‹œ ì£¼ì˜ì‚¬í•­ì„ ì¶”ê°€í•˜ì„¸ìš”
5. ë¬¸ì œê°€ ê³„ì†ë˜ë©´ ê³ ê°ì„¼í„° ì•ˆë‚´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”
6. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”"""


# ==================== RAG + ìºì‹œ ì§€ì‹ ì„œë¹„ìŠ¤ ====================

class CachedRAGKnowledgeService:
    """
    ìºì‹œ ì‹œìŠ¤í…œì´ ì¶”ê°€ëœ RAG
    
    ì›Œí¬í”Œë¡œìš°:
    1. ìºì‹œ í™•ì¸ â†’ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ìŒ)
    2. ì—†ìœ¼ë©´ RAG í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    3. ë‹µë³€ ìƒì„± í›„ ìºì‹œì— ì €ì¥ (pending ìƒíƒœ)
    4. ì‚¬ìš©ì í”¼ë“œë°± ë°›ìœ¼ë©´ ìºì‹œ ì—…ë°ì´íŠ¸
    """
    
    def __init__(self, 
<<<<<<< HEAD
<<<<<<< HEAD
                 csv_path: str = "backend/data/faq_database.csv",
                 cache_file: str = "backend/data/answer_cache.json",
=======
                 csv_path: str = "data/faq_database.csv",
>>>>>>> origin/kyj/transaction
=======
                 csv_path: str = "backend/data/faq_database_48.csv",
>>>>>>> origin/feat/ohs-rag
                 model_name: str = "jhgan/ko-sroberta-multitask",
                 enable_conversation: bool = True,
                 enable_cache: bool = True,
                 api_key: str = None):
        
        logger.info("=" * 60)
        logger.info("BíŒŒíŠ¸: ìºì‹œ + RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info("=" * 60)
        
        self.enable_cache = enable_cache
        
        # ìºì‹œ (NEW!)
        if enable_cache:
            self.cache = AnswerCache(cache_file)
        else:
            self.cache = None
        
        # ëŒ€í™” ë§¥ë½
        if enable_conversation:
            self.conversation = ConversationManager()
            logger.info("  âœ… ëŒ€í™” ë§¥ë½ ê´€ë¦¬ í™œì„±í™”")
        else:
            self.conversation = None
        
        # LLM Agent
        self.llm_agent = LLMAgent(api_key=api_key, max_retries=3)
        
        # ì„ë² ë”© ëª¨ë¸
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        logger.info(f"  âœ… ì„ë² ë”© ì°¨ì›: {self.dimension}")
        
        # FAQ ë°ì´í„°
        self.faq_df = self._load_csv(csv_path)
        self.index = self._build_index()
        
        logger.info("âœ… ìºì‹œ + RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\n")
    
    def _load_csv(self, csv_path) -> pd.DataFrame:
        """CSV ë¡œë“œ"""
        csv_file = Path(csv_path)
        
        if not csv_file.exists():
            raise FileNotFoundError(f"FAQ íŒŒì¼ ì—†ìŒ: {csv_path}")
        
        df = pd.read_csv(csv_file, encoding='utf-8')
        logger.info(f"  âœ… CSV ë¡œë“œ: {len(df)}ê°œ FAQ")
        
        required = ['id', 'category', 'question', 'answer']
        missing = [col for col in required if col not in df.columns]
        if missing:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        
        return df
    
    def _build_index(self):
        """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        texts = []
        for idx, row in self.faq_df.iterrows():
            text = row['question']
            if 'keywords' in self.faq_df.columns and pd.notna(row['keywords']):
                text += " " + row['keywords'].replace(',', ' ')
            texts.append(text)
        
        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)
        faiss.normalize_L2(embeddings)
        
        index = faiss.IndexFlatIP(self.dimension)
        index.add(embeddings)
        
        logger.info(f"  âœ… FAISS ì¸ë±ìŠ¤: {index.ntotal}ê°œ ë²¡í„°")
        return index
    
    def search_knowledge(self, 
                        query: str, 
                        category: str = None,
                        session_id: str = None) -> Dict:
        """
        ìºì‹œ + RAG ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰
        
        í”„ë¡œì„¸ìŠ¤:
        1. ìºì‹œ í™•ì¸ â†’ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
        2. ì—†ìœ¼ë©´ RAG ì‹¤í–‰
        3. ë‹µë³€ì„ ìºì‹œì— ì €ì¥ (pending)
        """
        original_query = query
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ê²€ìƒ‰ ì‹œì‘: '{query}'")
        logger.info(f"{'='*60}")
        
        # Step 0: ìºì‹œ í™•ì¸ (NEW!)
        if self.enable_cache and self.cache:
            cached_answer = self.cache.get(query, category)
            
            if cached_answer:
                # ìºì‹œ íˆíŠ¸! LLM í˜¸ì¶œ ì—†ì´ ì¦‰ì‹œ ë°˜í™˜
                self.cache.increment_hit_count(query, category)
                
                logger.info("  ğŸ’¾ ìºì‹œì—ì„œ ë‹µë³€ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ìŒ)")
                
                # ëŒ€í™” ê¸°ë¡
                if self.conversation and session_id:
                    self.conversation.add_turn(
                        session_id=session_id,
                        user_query=original_query,
                        bot_response=cached_answer['answer'],
                        from_cache=True
                    )
                
                return {
                    "answer": cached_answer['answer'],
                    "confidence": 1.0,  # ìºì‹œëœ ë‹µë³€ì€ ê²€ì¦ë¨
                    "from_cache": True,
                    "cache_verified": cached_answer.get('verified', False),
                    "cache_hit_count": cached_answer.get('hit_count', 0),
                    "used_llm": False  # LLM í˜¸ì¶œ ì•ˆ í•¨!
                }
        
        # Step 1: ëŒ€í™” ë§¥ë½ í•´ê²°
        if self.conversation and session_id:
            resolved_query = self.conversation.resolve_references(session_id, query)
            if resolved_query != query:
                query = resolved_query
        
        # Step 2-4: RAG í”„ë¡œì„¸ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
        results = self._search_faq(query, category, top_k=3)
        
        if not results:
            return {
                "answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "confidence": 0.0
            }
        
        best_score = results[0]['similarity_score']
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        retrieved_context = self._build_retrieved_context(results)
        conversation_context = ""
        if self.conversation and session_id:
            conversation_context = self.conversation.build_context_prompt(session_id)
        
        final_prompt = self._chain_prompts(query, retrieved_context, conversation_context)
        
        # LLM í˜¸ì¶œ
        try:
            logger.info("[Generation] LLM ë‹µë³€ ìƒì„±")
            answer = self.llm_agent.generate_with_retry(prompt=final_prompt)
            
            # Step 5: ìºì‹œì— ì €ì¥ (pending ìƒíƒœ) (NEW!)
            if self.enable_cache and self.cache:
                self.cache.add(
                    query=original_query,
                    answer=answer,
                    category=category,
                    verified=False,  # ì•„ì§ ê²€ì¦ ì•ˆ ë¨
                    metadata={
                        'faq_ids': [r['faq_id'] for r in results],
                        'confidence': best_score
                    }
                )
            
            # ëŒ€í™” ê¸°ë¡
            suggested_action = self._extract_first_action(answer)
            if self.conversation and session_id:
                self.conversation.add_turn(
                    session_id=session_id,
                    user_query=original_query,
                    bot_response=answer,
                    suggested_action=suggested_action,
                    faq_ids=[r['faq_id'] for r in results],
                    from_cache=False
                )
            
            return {
                "answer": answer,
                "confidence": best_score,
                "from_cache": False,
                "used_llm": True,
                "matched_faq_ids": [r['faq_id'] for r in results],
                "context_used": original_query != query,
                "pending_verification": True  # ì‚¬ìš©ì í”¼ë“œë°± ëŒ€ê¸° ì¤‘
            }
            
        except Exception as e:
            logger.error(f"âŒ LLM ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "answer": results[0]['answer'],
                "confidence": best_score,
                "error": str(e)
            }
    
    def submit_feedback(self, 
                       query: str, 
                       category: str = None,
                       is_helpful: bool = True,
                       feedback_score: int = 5,
                       reason: str = None):
        """
        ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ (NEW!)
        
        Args:
            query: ì§ˆë¬¸
            category: ì¹´í…Œê³ ë¦¬
            is_helpful: ë‹µë³€ì´ ë„ì›€ì´ ë˜ì—ˆëŠ”ì§€
            feedback_score: ì ìˆ˜ (1-5)
            reason: ê±°ë¶€ ì´ìœ  (is_helpful=Falseì¼ ë•Œ)
        """
        if not self.enable_cache or not self.cache:
            logger.warning("ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return
        
        if is_helpful:
            self.cache.verify(query, category, feedback_score)
            logger.info(f"  âœ… ê¸ì • í”¼ë“œë°±: {query[:30]}... (ì ìˆ˜: {feedback_score})")
        else:
            self.cache.reject(query, category, reason)
            logger.info(f"  âŒ ë¶€ì • í”¼ë“œë°±: {query[:30]}...")
    
    def get_cache_stats(self) -> Dict:
        """ìºì‹œ í†µê³„ ì¡°íšŒ (NEW!)"""
        if not self.enable_cache or not self.cache:
            return {'cache_enabled': False}
        
        stats = self.cache.get_stats()
        stats['cache_enabled'] = True
        return stats
    
    def _search_faq(self, query: str, category: str = None, top_k: int = 3) -> List[Dict]:
        """FAQ ê²€ìƒ‰"""
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.index.search(query_embedding, min(top_k * 2, len(self.faq_df)))
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if score < 0.2:
                continue
            
            faq_row = self.faq_df.iloc[idx]
            
            if category and faq_row['category'] != category:
                continue
            
            results.append({
                'faq_id': faq_row['id'],
                'category': faq_row['category'],
                'question': faq_row['question'],
                'answer': faq_row['answer'],
                'similarity_score': float(score)
            })
            
            if len(results) >= top_k:
                break
        
        return results
    
    def _build_retrieved_context(self, results: List[Dict]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        if not results:
            return ""
        
        context = "[ê²€ìƒ‰ëœ ê´€ë ¨ FAQ]\n\n"
        
        for i, faq in enumerate(results, 1):
            context += f"FAQ {i} (ìœ ì‚¬ë„: {faq['similarity_score']:.2f}):\n"
            context += f"ì§ˆë¬¸: {faq['question']}\n"
            context += f"ë‹µë³€: {faq['answer']}\n\n"
        
        return context
    
    def _chain_prompts(self, user_query: str, retrieved_context: str, conversation_context: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ì²´ì¸"""
        prompt = ""
        
        if retrieved_context:
            prompt += retrieved_context
            prompt += "---\n\n"
        
        if conversation_context:
            prompt += conversation_context
            prompt += "---\n\n"
        
        prompt += f"[ê³ ê° ì§ˆë¬¸]\n{user_query}\n\n"
        prompt += "[ì§€ì‹œì‚¬í•­]\n"
        prompt += "ìœ„ì˜ FAQì™€ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        
        return prompt
    
    def _extract_first_action(self, answer: str) -> Optional[str]:
        """ì²« ë²ˆì§¸ ì¡°ì¹˜ ì¶”ì¶œ"""
        match = re.search(r'1\.\s*([^\n]+)', answer)
        if match:
            action = match.group(1).strip()
            action = re.sub(r'\([^)]*\)', '', action).strip()
            return action[:50]
        return None
<<<<<<< HEAD
=======
    
    def _search_faq(self, query: str, category: str = None, top_k: int = 3) -> List[Dict]:
        """FAQ ê²€ìƒ‰ (í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… í¬í•¨)"""
        if self.index is None:
            return []
        
        try:
            query_embedding = self.model.encode([query], convert_to_numpy=True)
            faiss.normalize_L2(query_embedding)
            
            scores, indices = self.index.search(query_embedding, min(top_k * 2, len(self.faq_df)))
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if score < 0.2:
                    continue
                
                faq_row = self.faq_df.iloc[idx]
                
                if category and faq_row['category'] != category:
                    continue
                
                final_score = float(score)
                
                # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… (ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì ìˆ˜ ë³´ì •)
                if 'keywords' in self.faq_df.columns and pd.notna(faq_row['keywords']):
                    keywords = [k.strip() for k in faq_row['keywords'].split(',')]
                    for kw in keywords:
                        if kw in query:
                            final_score += 0.1  # í‚¤ì›Œë“œ ì¼ì¹˜ ì‹œ ë¶€ìŠ¤íŒ…
                            break
                
                results.append({
                    'faq_id': faq_row['id'],
                    'category': faq_row['category'],
                    'question': faq_row['question'],
                    'answer': faq_row['answer'],
                    'similarity_score': min(final_score, 1.0)
                })
                
                if len(results) >= top_k:
                    break
            
            # ë¶€ìŠ¤íŒ…ëœ ì ìˆ˜ë¡œ ì¬ì •ë ¬
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            return results
            
        except Exception as e:
            logger.error(f"FAQ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_ai_answer(self, query: str, faq_results: List[Dict]) -> str:
        """AI ë‹µë³€ ìƒì„±"""
        if not self.ai_client:
            return "AI ë‹µë³€ ìƒì„± ë¶ˆê°€"
        
        try:
            context = ""
            if faq_results:
                context = "ì°¸ê³  FAQ:\n"
                for faq in faq_results[:2]:
                    context += f"Q: {faq['question']}\nA: {faq['answer'][:100]}...\n\n"
            
            response = self.ai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ì¹œì ˆí•œ ê³ ê° ì§€ì› AIì…ë‹ˆë‹¤."},
                    {"role": "user", "content": f"{context}\nì§ˆë¬¸: {query}"}
                ],
                temperature=0.7,
                max_tokens=400
            )
            
            answer = response.choices[0].message.content.strip()
            answer += "\n\nğŸ¤– (AI ìƒì„± ë‹µë³€)"
            return answer
            
        except Exception as e:
            logger.error(f"AI ìƒì„± ì‹¤íŒ¨: {e}")
            return "AI ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_out_of_scope_message(self) -> str:
        """ë²”ìœ„ ë°– ë©”ì‹œì§€"""
        return """ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ìŒ ë¶„ì•¼ë§Œ ì§€ì› ê°€ëŠ¥í•©ë‹ˆë‹¤:
>>>>>>> origin/feat/ohs-rag


# í¸ì˜ë¥¼ ìœ„í•œ alias
KnowledgeService = CachedRAGKnowledgeService


# ==================== í…ŒìŠ¤íŠ¸ ====================

def test_cache_system():
    """ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 70)
    print("ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    service = CachedRAGKnowledgeService(
        csv_path="faq_database_48.csv",
        enable_conversation=True,
        enable_cache=True
    )
    
    session_id = "test_001"
    query = "ì¸í„°ë„·ì´ ì•ˆ ë¼ìš”"
    
    # ì²« ë²ˆì§¸ ìš”ì²­ (ìºì‹œ ë¯¸ìŠ¤)
    print("\n[í…ŒìŠ¤íŠ¸ 1] ì²« ë²ˆì§¸ ìš”ì²­ (ìºì‹œ ë¯¸ìŠ¤)")
    result1 = service.search_knowledge(query, "tech_support", session_id)
    
    print(f"ìºì‹œ ì‚¬ìš©: {result1.get('from_cache')}")
    print(f"LLM ì‚¬ìš©: {result1.get('used_llm')}")
    print(f"ë‹µë³€: {result1['answer'][:100]}...")
    
    # ê¸ì • í”¼ë“œë°±
    print("\n[í…ŒìŠ¤íŠ¸ 2] ê¸ì • í”¼ë“œë°± ì œì¶œ")
    service.submit_feedback(
        query=query,
        category="tech_support",
        is_helpful=True,
        feedback_score=5
    )
    
    # ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œ íˆíŠ¸!)
    print("\n[í…ŒìŠ¤íŠ¸ 3] ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œ íˆíŠ¸)")
    result2 = service.search_knowledge(query, "tech_support", session_id)
    
    print(f"ìºì‹œ ì‚¬ìš©: {result2.get('from_cache')}")  # True!
    print(f"LLM ì‚¬ìš©: {result2.get('used_llm')}")    # False!
    print(f"ë‹µë³€: {result2['answer'][:100]}...")
    
    # ìºì‹œ í†µê³„
    print("\n[ìºì‹œ í†µê³„]")
    stats = service.get_cache_stats()
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    print("\n" + "=" * 70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)


if __name__ == "__main__":
    test_cache_system()