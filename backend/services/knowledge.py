"""
BíŒŒíŠ¸: ìºì‹œ ì‹œìŠ¤í…œì´ ì¶”ê°€ëœ RAG
- ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ìºì‹œ
- ê²€ì¦ëœ ë‹µë³€ ì¬ì‚¬ìš©
- LLM ë¹„ìš© ì ˆê°
"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from datetime import datetime
import logging
import os
import re
import time
import json
import hashlib
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================== ìºì‹œ ê´€ë¦¬ì ====================

class AnswerCache:
    """
    ê²€ì¦ëœ ë‹µë³€ ìºì‹œ ì‹œìŠ¤í…œ
    
    ê¸°ëŠ¥:
    1. ì§ˆë¬¸-ë‹µë³€ ìŒ ì €ì¥
    2. ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ìºì‹±
    3. ìºì‹œ íˆíŠ¸ ì‹œ ì¦‰ì‹œ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ìŒ)
    """
    
    def __init__(self, cache_file: str = "backend/data/answer_cache.json"):
        """ìºì‹œ íŒŒì¼ ì´ˆê¸°í™” - ì—¬ëŸ¬ ê²½ë¡œ íƒìƒ‰"""
        base_dir = Path(__file__).parent.parent
        
        # ê°€ëŠ¥í•œ ê²½ë¡œë“¤ ìˆœì„œëŒ€ë¡œ íƒìƒ‰
        search_paths = [
            Path(cache_file),
            base_dir / "data" / "answer_cache.json",
            Path(os.getcwd()) / "backend" / "data" / "answer_cache.json",
            Path(os.getcwd()) / "data" / "answer_cache.json"
        ]
        
        # ì¡´ì¬í•˜ëŠ” íŒŒì¼ ì°¾ê¸° ë˜ëŠ” ì²« ë²ˆì§¸ ê²½ë¡œ ì‚¬ìš©
        self.cache_file = None
        for path in search_paths:
            if path.exists():
                self.cache_file = path
                break
        
        if not self.cache_file:
            self.cache_file = search_paths[1]
            
        self.cache = self._load_cache()
        self.embeddings_cache = {}
        logger.info(f"  âœ… ë‹µë³€ ìºì‹œ ì´ˆê¸°í™” ({len(self.cache)}ê°œ ì €ì¥ë¨)")
    
    def _load_cache(self) -> Dict:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        try:
            self.cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_query_hash(self, query: str, category: str = None) -> str:
        """ì§ˆë¬¸ì˜ í•´ì‹œê°’ ìƒì„± - ê³µë°±/ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""
        clean_query = query.strip().lower().replace(" ", "")
        key = f"{category}:{clean_query}" if category else clean_query
        return hashlib.md5(key.encode()).hexdigest()
    
    def get(self, query: str, category: str = None, similarity_threshold: float = 0.95) -> Optional[Dict]:
        """ìºì‹œì—ì„œ ë‹µë³€ ì¡°íšŒ"""
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            cached_item = self.cache[query_hash]
            
            if cached_item.get('verified', False):
                logger.info(f"  ğŸ’¾ ìºì‹œ íˆíŠ¸! (ì •í™•í•œ ë§¤ì¹­)")
                cached_item['cache_hit'] = True
                cached_item['cache_type'] = 'exact'
                return cached_item
        
        return None
    
    def add(self, query: str, answer: str, category: str = None, verified: bool = False, feedback_score: int = 0, metadata: Dict = None) -> str:
        """ìºì‹œì— ë‹µë³€ ì¶”ê°€"""
        query_hash = self._get_query_hash(query, category)
        
        self.cache[query_hash] = {
            'query': query,
            'answer': answer,
            'category': category,
            'verified': verified,
            'feedback_score': feedback_score,
            'created_at': datetime.now().isoformat(),
            'hit_count': 0,
            'metadata': metadata or {}
        }
        
        self._save_cache()
        logger.info(f"  ğŸ’¾ ìºì‹œ ì¶”ê°€: {query[:30]}... (verified={verified})")
        
        return query_hash
    
    def verify(self, query: str, category: str = None, feedback_score: int = 5):
        """ì‚¬ìš©ìê°€ ë‹µë³€ì„ ìŠ¹ì¸"""
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            self.cache[query_hash]['verified'] = True
            self.cache[query_hash]['feedback_score'] = feedback_score
            self.cache[query_hash]['verified_at'] = datetime.now().isoformat()
            
            self._save_cache()
            logger.info(f"  âœ… ë‹µë³€ ìŠ¹ì¸: {query[:30]}... (ì ìˆ˜: {feedback_score})")
        else:
            logger.warning(f"  âš ï¸  ìºì‹œì— ì—†ëŠ” ì§ˆë¬¸: {query[:30]}...")
    
    def reject(self, query: str, category: str = None, reason: str = None):
        """ì‚¬ìš©ìê°€ ë‹µë³€ì„ ê±°ë¶€"""
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            self.cache[query_hash]['verified'] = False
            self.cache[query_hash]['rejected'] = True
            self.cache[query_hash]['rejected_at'] = datetime.now().isoformat()
            self.cache[query_hash]['rejection_reason'] = reason
            
            self._save_cache()
            logger.info(f"  âŒ ë‹µë³€ ê±°ë¶€: {query[:30]}...")
    
    def increment_hit_count(self, query: str, category: str = None):
        """ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€"""
        query_hash = self._get_query_hash(query, category)
        
        if query_hash in self.cache:
            self.cache[query_hash]['hit_count'] = self.cache[query_hash].get('hit_count', 0) + 1
            self.cache[query_hash]['last_used'] = datetime.now().isoformat()
            self._save_cache()
    
    def get_stats(self) -> Dict:
        """ìºì‹œ í†µê³„"""
        total = len(self.cache)
        verified = sum(1 for item in self.cache.values() if item.get('verified'))
        rejected = sum(1 for item in self.cache.values() if item.get('rejected'))
        pending = total - verified - rejected
        
        total_hits = sum(item.get('hit_count', 0) for item in self.cache.values())
        
        return {
            'total_cached': total,
            'verified': verified,
            'rejected': rejected,
            'pending': pending,
            'total_cache_hits': total_hits,
            'cache_hit_rate': total_hits / max(total, 1)
        }


# ==================== ëŒ€í™” ë§¥ë½ ê´€ë¦¬ì ====================

class ConversationManager:
    """ëŒ€í™” ë§¥ë½ ê´€ë¦¬"""
    
    def __init__(self):
        self.sessions = {}
    
    def add_turn(self, session_id: str, user_query: str, bot_response: str, suggested_action: str = None, faq_ids: List[str] = None, from_cache: bool = False):
        """ëŒ€í™” í„´ ì¶”ê°€"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {
                'history': [],
                'current_issue': None,
                'tried_solutions': [],
                'last_suggestion': None,
                'last_query': None,  # âœ… ì¶”ê°€: ë§ˆì§€ë§‰ ì§ˆë¬¸ ì €ì¥
                'created_at': datetime.now()
            }
        
        self.sessions[session_id]['history'].append({
            'timestamp': datetime.now(),
            'user_query': user_query,
            'bot_response': bot_response,
            'suggested_action': suggested_action,
            'faq_ids': faq_ids or [],
            'from_cache': from_cache
        })
        
        if suggested_action:
            self.sessions[session_id]['last_suggestion'] = suggested_action
            if suggested_action not in self.sessions[session_id]['tried_solutions']:
                self.sessions[session_id]['tried_solutions'].append(suggested_action)
        
        # âœ… ë§ˆì§€ë§‰ ì§ˆë¬¸ ì €ì¥
        self.sessions[session_id]['last_query'] = user_query
        
        if not self.sessions[session_id]['current_issue']:
            self.sessions[session_id]['current_issue'] = self._extract_issue(user_query)
    
    def _extract_issue(self, query: str) -> str:
        """í˜„ì¬ ë¬¸ì œ ì¶”ì¶œ"""
        issues = {
            'ë¡œê·¸ì¸': 'login_issue',
            'ì¸í„°ë„·': 'internet_issue',
            'ì™€ì´íŒŒì´': 'wifi_issue',
            'ì•±': 'app_issue',
            'ëŠë¦¼': 'slow_issue',
            'ì²­êµ¬': 'billing_issue',
            'ì£¼ë¬¸': 'order_issue'
        }
        
        for keyword, issue in issues.items():
            if keyword in query:
                return issue
        return 'general_issue'
    
    def resolve_references(self, session_id: str, query: str) -> str:
        """
        ì§€ì‹œ ëŒ€ëª…ì‚¬ í•´ê²° - ê°œì„  ë²„ì „
        
        "ê·¸ê±° í–ˆëŠ”ë°ë„ ì•ˆ ë¼ìš”" â†’ "ë¡œê·¸ì¸ì´ ì•ˆ ë¼ìš”" (ì›ë˜ ì§ˆë¬¸ìœ¼ë¡œ ë˜ëŒë¦¼)
        """
        if session_id not in self.sessions:
            return query
        
        context = self.sessions[session_id]
        
        # âœ… "ê·¸ê±°/ì´ê±°/ì•ˆë¼ìš”" ê°™ì€ ì°¸ì¡°ì–´ ê°ì§€
        has_reference = any(ref in query for ref in ['ê·¸ê±°', 'ê·¸ê²ƒ', 'ì´ê±°', 'ì´ê²ƒ', 'ì•ˆë¼', 'ì•ˆ ë¼'])
        
        if has_reference and context.get('last_query'):
            # âœ… ì›ë˜ ì§ˆë¬¸ìœ¼ë¡œ ë˜ëŒë¦¼ (ì œì•ˆì‚¬í•­ ì¶”ê°€ëŠ” ë§¥ë½ í”„ë¡¬í”„íŠ¸ì—ì„œ)
            resolved = context['last_query']
            
            # ë‹¨, "ë‹¤ì‹œ" ë˜ëŠ” "ì—¬ì „íˆ" ê°™ì€ í‚¤ì›Œë“œ ì¶”ê°€
            if 'ë‹¤ì‹œ' in query or 'ì—¬ì „íˆ' in query or 'ê³„ì†' in query:
                resolved = f"{resolved} (ë‹¤ì‹œ ì‹œë„í•´ë„ ì•ˆë¨)"
            
            logger.info(f"[ë§¥ë½ í•´ê²°] '{query}' â†’ '{resolved}'")
            return resolved
        
        # âœ… ê¸°ì¡´ ë¡œì§ (ë‹¨ì–´ ì¹˜í™˜)
        if context.get('last_suggestion'):
            references = {
                'ê·¸ê±°': context['last_suggestion'],
                'ê·¸ê²ƒ': context['last_suggestion'],
                'ì´ê±°': context['last_suggestion'],
                'ì´ê²ƒ': context['last_suggestion'],
            }
            
            resolved = query
            for ref, actual in references.items():
                if ref in query:
                    resolved = resolved.replace(ref, actual)
                    if actual not in context['tried_solutions']:
                        context['tried_solutions'].append(actual)
            
            if resolved != query:
                logger.info(f"[ë§¥ë½ í•´ê²°] '{query}' â†’ '{resolved}'")
                return resolved
        
        return query
    
    def build_context_prompt(self, session_id: str) -> str:
        """ëŒ€í™” ë§¥ë½ì„ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜ - ê°•í™” ë²„ì „"""
        if session_id not in self.sessions:
            return ""
        
        context = self.sessions[session_id]
        if not context['tried_solutions']:
            return ""
        
        prompt = "\n[ì´ì „ ëŒ€í™” ë§¥ë½]\n"
        prompt += f"- í˜„ì¬ ë¬¸ì œ: {context['current_issue']}\n"
        
        # ë¬¸ì œ ëª…í™•í™”
        if context['current_issue'] == 'login_issue':
            prompt += "  ğŸ“Œ ë¡œê·¸ì¸ ìì²´ê°€ ì•ˆë˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤ (ìë™ ë¡œê·¸ì¸ ë¬¸ì œ ì•„ë‹˜!)\n"
        
        prompt += f"- ê³ ê°ì´ ì´ë¯¸ ì‹œë„í•œ ë°©ë²•:\n"
        for i, solution in enumerate(context['tried_solutions'], 1):
            prompt += f"  {i}. {solution}\n"
        
        prompt += "\nâš ï¸ ì¤‘ìš”: ìœ„ ë°©ë²•ë“¤ì€ ì´ë¯¸ ì‹œë„í–ˆìœ¼ë¯€ë¡œ **ì ˆëŒ€ ë‹¤ì‹œ ì œì•ˆí•˜ì§€ ë§ˆì„¸ìš”**.\n"
        prompt += "âš ï¸ ì™„ì „íˆ ë‹¤ë¥¸ í•´ê²°ì±…ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n"
        prompt += "âš ï¸ ìœ ì‚¬í•œ ë°©ë²•ë„ ì•ˆ ë©ë‹ˆë‹¤ (ì˜ˆ: ì¿ í‚¤ ì‚­ì œë¥¼ í–ˆìœ¼ë©´, ìºì‹œ ì‚­ì œë„ ì´ë¯¸ í–ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ).\n\n"
        
        return prompt


# ==================== Agent (ì¬ì‹œë„ ë¡œì§) ====================

class LLMAgent:
    """LLM í˜¸ì¶œì„ ë‹´ë‹¹í•˜ëŠ” Agent"""
    
    def __init__(self, api_key: str = None, max_retries: int = 3):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.max_retries = max_retries
        self.client = None
        
        if self.api_key:
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=self.api_key)
                logger.info("  âœ… LLM Agent ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"  âŒ LLM Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def generate_with_retry(self, prompt: str, system_prompt: str = None, temperature: float = 0.7, max_tokens: int = 500) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” LLM í˜¸ì¶œ"""
        if not self.client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        system_prompt = system_prompt or self._get_default_system_prompt()
        
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.info(f"  ğŸ¤– LLM í˜¸ì¶œ ì‹œë„ {attempt}/{self.max_retries}")
                
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                answer = response.choices[0].message.content.strip()
                logger.info(f"  âœ… LLM í˜¸ì¶œ ì„±ê³µ (ê¸¸ì´: {len(answer)}ì)")
                
                return answer
                
            except Exception as e:
                logger.warning(f"  âš ï¸  LLM í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt}): {e}")
                
                if attempt < self.max_retries:
                    wait_time = 2 ** attempt
                    logger.info(f"  â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"  âŒ LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨")
                    raise Exception(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def _get_default_system_prompt(self) -> str:
        """ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ê°•í™” ë²„ì „"""
        return """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ê³ ê° ì§€ì› AIì…ë‹ˆë‹¤.

ë‹µë³€ ê·œì¹™:
1. **ìµœìš°ì„ **: ê³ ê°ì´ ì´ë¯¸ ì‹œë„í•œ ë°©ë²•ì€ ì ˆëŒ€ ë‹¤ì‹œ ì œì•ˆí•˜ì§€ ë§ˆì„¸ìš”
2. ê³ ê°ì˜ ì •í™•í•œ ë¬¸ì œë¥¼ íŒŒì•…í•˜ì„¸ìš”
   - "ë¡œê·¸ì¸ì´ ì•ˆë¨" â‰  "ìë™ ë¡œê·¸ì¸ ë¬¸ì œ"
   - "ê²°ì œì°½ì´ ì•ˆ ëœ¸" â‰  "ê²°ì œ ì™„ë£Œ ì•ˆë¨"
3. ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš” (1, 2, 3...)
4. ê¸°ìˆ  ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
5. í•„ìš”ì‹œ ì£¼ì˜ì‚¬í•­ì„ ì¶”ê°€í•˜ì„¸ìš”
6. ë¬¸ì œê°€ ê³„ì†ë˜ë©´ ê³ ê°ì„¼í„° ì•ˆë‚´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”
7. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”

ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ:
- ì´ë¯¸ ì‹œë„í•œ ë°©ë²•ì„ ë‹¤ì‹œ ì œì•ˆí•˜ê¸°
- ìœ ì‚¬í•œ ë°©ë²•ë„ ì•ˆ ë¨ (ì¿ í‚¤ ì‚­ì œ í–ˆìœ¼ë©´ ìºì‹œ ì‚­ì œë„ í–ˆì„ ê²ƒ)
- FAQì— ìˆì–´ë„, ì´ë¯¸ ì‹œë„í–ˆìœ¼ë©´ ì ˆëŒ€ ì–¸ê¸‰ ê¸ˆì§€
- ë¬¸ì œë¥¼ ì˜ëª» ì´í•´í•˜ê¸° (ìë™ ë¡œê·¸ì¸ vs ì¼ë°˜ ë¡œê·¸ì¸)"""


# ==================== RAG + ìºì‹œ ì§€ì‹ ì„œë¹„ìŠ¤ ====================

class CachedRAGKnowledgeService:
    """
    ìºì‹œ ì‹œìŠ¤í…œì´ ì¶”ê°€ëœ RAG
    
    ì›Œí¬í”Œë¡œìš°:
    1. ìºì‹œ í™•ì¸ â†’ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ìŒ)
    2. ì—†ìœ¼ë©´ RAG í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    3. ë‹µë³€ ìƒì„± í›„ ìºì‹œì— ì €ì¥ (pending ìƒíƒœ)
    4. ì‚¬ìš©ì í”¼ë“œë°± ë°›ìœ¼ë©´ ìºì‹œ ì—…ë°ì´íŠ¸
    """
    
    def __init__(self, 
                 csv_path: str = "backend/data/faq_database.csv",
                 cache_file: str = "backend/data/answer_cache.json",
                 model_name: str = "jhgan/ko-sroberta-multitask",
                 enable_conversation: bool = True,
                 enable_cache: bool = True,
                 api_key: str = None):
        
        logger.info("=" * 60)
        logger.info("BíŒŒíŠ¸: ìºì‹œ + RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info("=" * 60)
        
        self.enable_cache = enable_cache
        self.enable_conversation = enable_conversation
        
        if enable_cache:
            self.cache = AnswerCache(cache_file)
        else:
            self.cache = None
        
        if enable_conversation:
            self.conversation = ConversationManager()
            logger.info("  âœ… ëŒ€í™” ë§¥ë½ ê´€ë¦¬ í™œì„±í™”")
        else:
            self.conversation = None
        
        self.llm_agent = LLMAgent(api_key=api_key, max_retries=3)
        
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        logger.info(f"  âœ… ì„ë² ë”© ì°¨ì›: {self.dimension}")
        
        self.faq_df = self._load_csv(csv_path)
        self.index = self._build_index()
        
        logger.info("âœ… ìºì‹œ + RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\n")
    
    def _load_csv(self, csv_path) -> pd.DataFrame:
        """CSV ë¡œë“œ - ì—¬ëŸ¬ ê²½ë¡œ íƒìƒ‰ + ì¤‘ë³µ ì œê±°"""
        base_dir = Path(__file__).parent.parent
        
        search_paths = [
            Path(csv_path),
            base_dir / "data" / "faq_database.csv",
            Path(os.getcwd()) / "backend" / "data" / "faq_database.csv",
            Path(os.getcwd()) / "data" / "faq_database.csv"
        ]
        
        csv_file = None
        for path in search_paths:
            if path.exists():
                csv_file = path
                break
        
        if not csv_file:
            raise FileNotFoundError(
                f"FAQ íŒŒì¼ ì—†ìŒ: {csv_path}\n"
                f"ê²€ìƒ‰í•œ ê²½ë¡œ: {[str(p) for p in search_paths]}"
            )
        
        df = pd.read_csv(csv_file, encoding='utf-8')
        
        # ì¤‘ë³µ ID ì œê±°
        original_len = len(df)
        df = df.drop_duplicates(subset=['id'], keep='first')
        
        if len(df) < original_len:
            logger.warning(f"  âš ï¸  ì¤‘ë³µ FAQ ì œê±°: {original_len}ê°œ â†’ {len(df)}ê°œ")
        
        logger.info(f"  âœ… CSV ë¡œë“œ: {len(df)}ê°œ FAQ")
        
        required = ['id', 'category', 'question', 'answer']
        missing = [col for col in required if col not in df.columns]
        if missing:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        
        return df
    
    def _build_index(self):
        """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        texts = []
        for idx, row in self.faq_df.iterrows():
            question = str(row['question']) if pd.notna(row['question']) else ""
            text = question
            
            if 'keywords' in self.faq_df.columns and pd.notna(row['keywords']):
                text += " " + str(row['keywords']).replace(',', ' ')
            
            texts.append(text)
        
        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)
        faiss.normalize_L2(embeddings)
        
        index = faiss.IndexFlatIP(self.dimension)
        index.add(embeddings)
        
        logger.info(f"  âœ… FAISS ì¸ë±ìŠ¤: {index.ntotal}ê°œ ë²¡í„°")
        return index
    
    # âœ… agent.py í˜¸í™˜ìš© - str ë°˜í™˜
    def search_knowledge(self, query: str, category: str = None, session_id: str = None) -> str:
        """agent.py í˜¸í™˜ìš© ë©”ì„œë“œ - ë¬¸ìì—´ ë°˜í™˜"""
        result = self._search_knowledge_internal(query, category, session_id)
        return result.get('answer', 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')

    # âœ… ì‹¤ì œ RAG ë¡œì§ - Dict ë°˜í™˜
    def _search_knowledge_internal(self, query: str, category: str = None, session_id: str = None) -> Dict:
        """ì‹¤ì œ RAG ì²˜ë¦¬ ë¡œì§"""
        original_query = query
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ê²€ìƒ‰ ì‹œì‘: '{query}'")
        logger.info(f"{'='*60}")
        
        # Step 0: ìºì‹œ í™•ì¸ (ì›ë˜ ì§ˆë¬¸ìœ¼ë¡œ)
        if self.enable_cache and self.cache:
            cached_answer = self.cache.get(original_query, category)
            
            if cached_answer:
                self.cache.increment_hit_count(original_query, category)
                logger.info("  ğŸ’¾ ìºì‹œì—ì„œ ë‹µë³€ ë°˜í™˜ (LLM í˜¸ì¶œ ì—†ìŒ)")
                
                if self.conversation and session_id:
                    self.conversation.add_turn(
                        session_id=session_id,
                        user_query=original_query,
                        bot_response=cached_answer['answer'],
                        from_cache=True
                    )
                
                return {
                    "answer": cached_answer['answer'],
                    "confidence": 1.0,
                    "from_cache": True,
                    "cache_verified": cached_answer.get('verified', False),
                    "cache_hit_count": cached_answer.get('hit_count', 0),
                    "used_llm": False
                }
        
        # Step 1: ëŒ€í™” ë§¥ë½ í•´ê²°
        if self.conversation and session_id:
            resolved_query = self.conversation.resolve_references(session_id, query)
            if resolved_query != query:
                logger.info(f"  ğŸ”„ ë§¥ë½ í•´ê²°ë¨: '{query}' â†’ '{resolved_query}'")
                query = resolved_query
        
        # Step 2: FAQ ê²€ìƒ‰
        results = self._search_faq(query, category, top_k=3)
        
        # Step 3: ê²€ìƒ‰ ê²°ê³¼ ì—†ì–´ë„ LLM í˜¸ì¶œ
        if not results:
            logger.warning("  âš ï¸  FAQ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€")
            retrieved_context = "[FAQ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ]\nì¼ë°˜ì ì¸ ì‡¼í•‘ëª° ê³ ê°ì§€ì› ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            best_score = 0.0
        else:
            retrieved_context = self._build_retrieved_context(results)
            best_score = results[0]['similarity_score']
            logger.info(f"  âœ… FAQ ê²€ìƒ‰ ì™„ë£Œ (Top ìœ ì‚¬ë„: {best_score:.2f})")
        
        # Step 4: í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        conversation_context = ""
        if self.conversation and session_id:
            conversation_context = self.conversation.build_context_prompt(session_id)
        
        final_prompt = self._chain_prompts(query, retrieved_context, conversation_context)
        
        # Step 5: LLM í˜¸ì¶œ
        try:
            logger.info("[Generation] LLM ë‹µë³€ ìƒì„±")
            answer = self.llm_agent.generate_with_retry(prompt=final_prompt)
            
            # ìºì‹œì— ì €ì¥ (ì›ë˜ ì§ˆë¬¸ìœ¼ë¡œ)
            if self.enable_cache and self.cache:
                self.cache.add(
                    query=original_query,
                    answer=answer,
                    category=category,
                    verified=False,
                    metadata={
                        'faq_ids': [r['faq_id'] for r in results] if results else [],
                        'confidence': best_score
                    }
                )
            
            # ëŒ€í™” ê¸°ë¡
            suggested_action = self._extract_first_action(answer)
            if self.conversation and session_id:
                self.conversation.add_turn(
                    session_id=session_id,
                    user_query=original_query,
                    bot_response=answer,
                    suggested_action=suggested_action,
                    faq_ids=[r['faq_id'] for r in results] if results else [],
                    from_cache=False
                )
            
            return {
                "answer": answer,
                "confidence": best_score,
                "from_cache": False,
                "used_llm": True,
                "matched_faq_ids": [r['faq_id'] for r in results] if results else [],
                "context_used": original_query != query,
                "pending_verification": True
            }
            
        except Exception as e:
            logger.error(f"âŒ LLM ìƒì„± ì‹¤íŒ¨: {e}")
            
            # LLM ì‹¤íŒ¨ ì‹œ fallback
            if results:
                return {
                    "answer": results[0]['answer'],
                    "confidence": best_score,
                    "error": str(e)
                }
            else:
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "confidence": 0.0,
                    "error": str(e)
                }
    
    def submit_feedback(self, query: str, category: str = None, is_helpful: bool = True, feedback_score: int = 5, reason: str = None):
        """ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ"""
        if not self.enable_cache or not self.cache:
            logger.warning("ìºì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return
        
        if is_helpful:
            self.cache.verify(query, category, feedback_score)
        else:
            self.cache.reject(query, category, reason)
    
    def get_cache_stats(self) -> Dict:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        if not self.enable_cache or not self.cache:
            return {'cache_enabled': False}
        
        stats = self.cache.get_stats()
        stats['cache_enabled'] = True
        return stats
    
    def _search_faq(self, query: str, category: str = None, top_k: int = 3) -> List[Dict]:
        """FAQ ê²€ìƒ‰"""
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_embedding)
        
        search_k = min(top_k * 5, len(self.faq_df))
        scores, indices = self.index.search(query_embedding, search_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if score < 0.1:
                continue
            
            faq_row = self.faq_df.iloc[idx]
            
            if category and faq_row['category'] != category:
                if score < 0.3:
                    continue
            
            results.append({
                'faq_id': faq_row['id'],
                'category': faq_row['category'],
                'question': faq_row['question'],
                'answer': faq_row['answer'],
                'similarity_score': float(score)
            })
            
            if len(results) >= top_k:
                break
        
        return results
    
    def _build_retrieved_context(self, results: List[Dict]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        if not results:
            return ""
        
        context = "[ê²€ìƒ‰ëœ ê´€ë ¨ FAQ]\n\n"
        
        for i, faq in enumerate(results, 1):
            context += f"FAQ {i} (ìœ ì‚¬ë„: {faq['similarity_score']:.2f}):\n"
            context += f"ì§ˆë¬¸: {faq['question']}\n"
            context += f"ë‹µë³€: {faq['answer']}\n\n"
        
        return context
    
    def _chain_prompts(self, user_query: str, retrieved_context: str, conversation_context: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ì²´ì¸"""
        prompt = ""
        
        if retrieved_context:
            prompt += retrieved_context
            prompt += "---\n\n"
        
        if conversation_context:
            prompt += conversation_context
            prompt += "---\n\n"
        
        prompt += f"[ê³ ê° ì§ˆë¬¸]\n{user_query}\n\n"
        prompt += "[ì§€ì‹œì‚¬í•­]\n"
        prompt += "ìœ„ì˜ FAQì™€ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        
        return prompt
    
    def _extract_first_action(self, answer: str) -> Optional[str]:
        """ì²« ë²ˆì§¸ ì¡°ì¹˜ ì¶”ì¶œ"""
        match = re.search(r'1\.\s*([^\n]+)', answer)
        if match:
            action = match.group(1).strip()
            action = re.sub(r'\([^)]*\)', '', action).strip()
            return action[:50]
        return None


# í¸ì˜ë¥¼ ìœ„í•œ alias
KnowledgeService = CachedRAGKnowledgeService