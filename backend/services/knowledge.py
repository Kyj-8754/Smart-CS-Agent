import json
import os
import hashlib
import logging
import re
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple

import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== ìºì‹œ ê´€ë¦¬ì ====================

class AnswerCache:
    def __init__(self, cache_file: str = "data/answer_cache.json"):
        self.cache_file = Path(cache_file)
        self.cache = self._load_cache()
        logger.info(f"âœ… ë‹µë³€ ìºì‹œ ì´ˆê¸°í™” ({len(self.cache)}ê°œ ì €ì¥ë¨)")
    
    def _load_cache(self) -> Dict:
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        try:
            self.cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_query_hash(self, query: str, category: str = None) -> str:
        key = f"{category}:{query}" if category else query
        return hashlib.md5(key.encode()).hexdigest()
    
    def get(self, query: str, category: str = None) -> Optional[Dict]:
        query_hash = self._get_query_hash(query, category)
        if query_hash in self.cache:
            item = self.cache[query_hash]
            if item.get('verified', False):
                logger.info("ğŸ’¾ ìºì‹œ íˆíŠ¸! (ê²€ì¦ëœ ë‹µë³€)")
                return item
        return None
    
    def add(self, query: str, answer: str, category: str = None, verified: bool = False, metadata: Dict = None):
        query_hash = self._get_query_hash(query, category)
        self.cache[query_hash] = {
            'query': query,
            'answer': answer,
            'category': category,
            'verified': verified,
            'created_at': datetime.now().isoformat(),
            'hit_count': 0,
            'metadata': metadata or {}
        }
        self._save_cache()
    
    def verify(self, query: str, category: str = None):
        query_hash = self._get_query_hash(query, category)
        if query_hash in self.cache:
            self.cache[query_hash]['verified'] = True
            self.cache[query_hash]['verified_at'] = datetime.now().isoformat()
            self._save_cache()

    def reject(self, query: str, category: str = None, reason: str = None):
        query_hash = self._get_query_hash(query, category)
        if query_hash in self.cache:
            self.cache[query_hash]['verified'] = False
            self.cache[query_hash]['rejected'] = True
            self._save_cache()

    def increment_hit_count(self, query: str, category: str = None):
        query_hash = self._get_query_hash(query, category)
        if query_hash in self.cache:
            self.cache[query_hash]['hit_count'] += 1
            self._save_cache()

    def get_stats(self) -> Dict:
        total = len(self.cache)
        verified = sum(1 for item in self.cache.values() if item.get('verified'))
        return {'total_cached': total, 'verified': verified}

# ==================== ëŒ€í™” ë§¥ë½ ê´€ë¦¬ì ====================

class ConversationManager:
    def __init__(self):
        self.sessions = {}
    
    def add_turn(self, session_id: str, user_query: str, bot_response: str, suggested_action: str = None, from_cache: bool = False):
        if session_id not in self.sessions:
            self.sessions[session_id] = {'history': [], 'tried_solutions': [], 'last_suggestion': None}
        
        self.sessions[session_id]['history'].append({'query': user_query, 'response': bot_response, 'from_cache': from_cache})
        if suggested_action:
            self.sessions[session_id]['last_suggestion'] = suggested_action
            self.sessions[session_id]['tried_solutions'].append(suggested_action)

    def resolve_references(self, session_id: str, query: str) -> str:
        if session_id not in self.sessions or not self.sessions[session_id]['last_suggestion']:
            return query
        ref = self.sessions[session_id]['last_suggestion']
        return query.replace("ê·¸ê±°", ref).replace("ì´ê±°", ref)

    def build_context_prompt(self, session_id: str) -> str:
        if session_id not in self.sessions or not self.sessions[session_id]['tried_solutions']:
            return ""
        solutions = ", ".join(self.sessions[session_id]['tried_solutions'])
        return f"\n[ì´ë¯¸ ì‹œë„í•œ ë°©ë²•: {solutions}] âš ï¸ ìœ„ ë°©ë²• ì™¸ì˜ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”.\n"

# ==================== LLM Agent ====================

class LLMAgent:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=self.api_key)
        except:
            self.client = None

    def generate_with_retry(self, prompt: str) -> str:
        if not self.client: return "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        res = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "system", "content": "ì¹œì ˆí•œ ìƒë‹´ì›ì…ë‹ˆë‹¤."}, {"role": "user", "content": prompt}]
        )
        return res.choices[0].message.content.strip()

# ==================== RAG + ìºì‹œ ì§€ì‹ ì„œë¹„ìŠ¤ ====================

class CachedRAGKnowledgeService:
    def __init__(self, csv_path: str, cache_file: str = "data/answer_cache.json", enable_cache: bool = True):
        self.enable_cache = enable_cache
        self.cache = AnswerCache(cache_file) if enable_cache else None
        self.conversation = ConversationManager()
        self.llm_agent = LLMAgent()
        self.model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        self.faq_df = pd.read_csv(csv_path)
        self.index = self._build_index()

    def _build_index(self):
        texts = (self.faq_df['question'] + " " + self.faq_df.get('keywords', '').fillna('')).tolist()
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        faiss.normalize_L2(embeddings)
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        return index

    def _search_faq(self, query: str, category: str = None, top_k: int = 3) -> List[Dict]:
        query_emb = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_emb)
        scores, indices = self.index.search(query_emb, min(top_k * 2, len(self.faq_df)))
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            row = self.faq_df.iloc[idx]
            if category and row['category'] != category: continue
            results.append({'faq_id': row['id'], 'question': row['question'], 'answer': row['answer'], 'similarity_score': float(score)})
            if len(results) >= top_k: break
        return results

    def search_knowledge(self, query: str, category: str = None, session_id: str = None) -> Dict:
        if self.enable_cache:
            cached = self.cache.get(query, category)
            if cached:
                self.cache.increment_hit_count(query, category)
                return {"answer": cached['answer'], "from_cache": True, "used_llm": False}

        resolved_query = self.conversation.resolve_references(session_id, query) if session_id else query
        faqs = self._search_faq(resolved_query, category)
        
        if not faqs: return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "confidence": 0.0}

        context = "\n".join([f"Q: {f['question']}\nA: {f['answer']}" for f in faqs])
        conv_prompt = self.conversation.build_context_prompt(session_id) if session_id else ""
        
        final_prompt = f"{context}\n{conv_prompt}\nì§ˆë¬¸: {resolved_query}"
        answer = self.llm_agent.generate_with_retry(final_prompt)

        if self.enable_cache:
            self.cache.add(query, answer, category)
        
        if session_id:
            self.conversation.add_turn(session_id, query, answer)

        return {"answer": answer, "from_cache": False, "used_llm": True, "confidence": faqs[0]['similarity_score']}

    def submit_feedback(self, query: str, category: str = None, is_helpful: bool = True):
        if self.cache:
            if is_helpful: self.cache.verify(query, category)
            else: self.cache.reject(query, category)

    def get_cache_stats(self):
        return self.cache.get_stats() if self.cache else {}

# ë³„ì¹­
KnowledgeService = CachedRAGKnowledgeService

# ==================== í…ŒìŠ¤íŠ¸ ====================

def test_cache_system():
    print("\n" + "=" * 70)
    print("ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # ë³¸ì¸ì˜ ì‹¤ì œ íŒŒì¼ëª… í™•ì¸ í•„ìˆ˜ 
    csv_file = "faq_database.csv" 
    if not os.path.exists(csv_file):
        with open(csv_file, "w", encoding="utf-8") as f:
            f.write("id,category,question,answer,keywords\nfaq_001,tech_support,ì¸í„°ë„· ì•ˆë¨,ê³µìœ ê¸°ë¥¼ ê»ë‹¤ ì¼œì„¸ìš”,ì¸í„°ë„·")

    service = KnowledgeService(csv_path=csv_file)
    session_id = "test_user_123"
    query = "ì¸í„°ë„·ì´ ì•ˆ ë¼ìš”"
    
    print("\n[í…ŒìŠ¤íŠ¸ 1] ì²« ë²ˆì§¸ ìš”ì²­ (ìºì‹œ ë¯¸ìŠ¤)")
    res1 = service.search_knowledge(query, "tech_support", session_id)
    print(f"ìºì‹œ ì‚¬ìš©: {res1['from_cache']} | ë‹µë³€: {res1['answer'][:50]}...")
    
    print("\n[í…ŒìŠ¤íŠ¸ 2] ê¸ì • í”¼ë“œë°± ì œì¶œ")
    service.submit_feedback(query, "tech_support", True)
    
    print("\n[í…ŒìŠ¤íŠ¸ 3] ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œ íˆíŠ¸)")
    res2 = service.search_knowledge(query, "tech_support", session_id)
    print(f"ìºì‹œ ì‚¬ìš©: {res2['from_cache']} | ë‹µë³€: {res2['answer'][:50]}...")

if __name__ == "__main__":
    test_cache_system()